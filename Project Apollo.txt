Project Apollo: Incremental Labeling Pipeline
This plan presents a simple, modular, and scalable pipeline for labeling Ethereum addresses using BigQuery. It is designed for clarity and easy communication during a panel interview.

1. Overall Approach
We will use Google BigQuery’s public Ethereum dataset as the core data source. Each label is derived through independent incremental pipelines, producing standardized outputs. The system outputs results to CSV (MVP) but can seamlessly expand to databases or S3/Parquet in the future.
Key principles: 
* Transparency: Each label has clear, rule-based logic.
* Incremental updates: Process only new blockchain data, append only new or changed labels.
* Extensibility: Adding a new label = adding a new pipeline with no disruption to existing ones.

2. Architecture



Data flow
1. Staging (stg_*): normalize Ethereum transactions, logs, and token transfers.
2. Label candidates (int_*_candidates): one incremental model per label (e.g., whale, NFT trader, DEX user). This step can be a combination of multiple logics and models. Each outputs a standard schema. 
3. Delta union (int_labels_long_delta): union of all new/changed rows across candidates, after anti-joining history to keep only fresh or higher-confidence tuples.
4. History (int_labels_main): MERGE deltas into the append-only history table with dedupe.
5. Snapshot (mart_labels): pivot to booleans for consumer simplicity.
6. Output: CSV now, database/S3 later.

3. Labeling Logic (MVP Examples)
* Whale: ETH balance > 1,000.
* NFT Trader: ?70% of interactions are with ERC-721 contracts.
* DEX User: interacted with known DEX router contracts.
* New Wallet: first transaction within the last 30 days.
Each produces standardized fields:
column_namedata_typeaddressSTRINGlabelSTRINGconfidenceFLOAT64created_atTIMESTAMPupdated_atTIMESTAMPsource_ruleSTRING
4. Incremental Design
* Partition pruning in each int_*_candidates: process only new block_timestamp partitions.
* Delta union: collect only new or higher-confidence rows across labels into int_labels_long_delta, partitioned by as_of_date, clustered by address,label.
* MERGE to history: write deltas into int_labels_main, deduping by (address,label,as_of_date) and preferring higher confidence.
* Idempotent reruns: delta is deterministic for a given run window; MERGE keys guarantee safe re-execution.
* Snapshot view: int_labels_wide always reflects latest labels.

5. Roadmap
1. MVP: staging + whale label + CSV output.
2. Add NFT trader and DEX user labels.
3. Add union + delta ? history logic.
4. Add wide snapshot + monitoring.
5. Export to Parquet/DB for scale.

































CHALLENGE

?? Data Engineer Take Home Challenge Overview At Nansen, we help crypto investors discover alpha and make better investment decisions using onchain data. Your job is not just to write code, but to think like a builder: understand the problem space, propose a scalable architecture, plan a path forward, and demonstrate hands-on skills by executing one part of the pipeline. Design and implement a data pipeline that generates valuable labels for blockchain addresses, showcasing your data engineering expertise, architectural thinking, and ability to build scalable solutions. This challenge evaluates your technical skills and your approach to handling ambiguous requirements - a key aspect of working at Nansen. You may use AI coding assistants to help with implementation. Problem Statement Overview Problem Statement What We’re Looking For Guidelines Tips Submission Data Engineer Take Home Challenge 1 ?? “Create a pipeline that produces valuable labels for blockchain addresses. These labels should be useful to an investor who wants to better understand interesting addresses on chain, and help them make smarter decisions.” We intentionally keep the problem open-ended to simulate what working at Nansen is like: we value initiative, structure, and clarity of thought in the face of ambiguity. WhatWe’re Looking For We’ll assess your submission across four dimensions: 1. Architectural Thinking Can you propose a clear, well-structured architecture for the pipeline? How do you handle scaling, maintainability, and data quality? 2. User-Centric Design Can you reflect on what makes a label valuable? Have you thought about the end-user and what they’d want? 3. Project Management Approach Can you break the problem into milestones, steps, and isolated tasks? Are you able to prioritise what should be built first? 4. Execution (Code Quality) Choose one part of your proposed system and implement it. This could be a data ingestion component, a labelling algorithm, a database model, or an API layer. Guidelines Data Engineer Take Home Challenge 2 Timebox your work to max 4 hours total. The deliverable is not meant to be exhaustive, use the task breakdown to highlight areas you would have dived deeper into You’re welcome to use any stack you’re comfortable with (e.g., Python, dbt, SQL, notebooks etc.) Tips Be opinionated, and explain your choices and trade-offs. Think from the user’s perspective, what would you want to see if you were using this data to trade or invest? You can find publicly available onchain data in BigQuery in bigquery-publicdata.crypto_ethereum If you looking for inspiration for labels, take a look at this blog post Submission Send your work as a GitHub repository with complete code and documentation. Your submission should include: A working code sample of one chosen part of the pipeline. README with: Setup instructions Architecture overview Technology choices and trade-offs Future improvements roadmap After we’ve reviewed your submission, we’ll invite you to a 60 minute walkthrough interview. You’ll get 20 minutes to present your work, before we’ll move into questions and discussion. Data Engineer Take Home Challenge 3 If you’re stuck or have questions, feel free to reach out. Data Engineer Take Home Challenge 4
